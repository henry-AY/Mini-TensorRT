# Mini-TensorRT

> [!IMPORTANT]
> This project is currently paused while I focus on open-source contribution work. Development will resume later in 2026.

Mini-TensorRT is a lightweight, real-time inference optimizer designed to accelerate LLMs through a custom C++ optimization pipeline. The long-term goal is to explore optimizations, fused kernels, and dynamic batching for faster transformer inference on available HuggingFace models.
